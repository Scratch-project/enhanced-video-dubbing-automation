{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Video Dubbing Automation\n",
    "\n",
    "## Arabic to English/German Video Dubbing Pipeline\n",
    "\n",
    "This notebook provides a complete automated pipeline for dubbing Arabic lecture/presentation videos into English and German, optimized for Kaggle's GPU environment.\n",
    "\n",
    "### Features:\n",
    "- **Step 0**: Environment setup and model caching\n",
    "- **Step 1**: Audio extraction and noise reduction\n",
    "- **Step 2**: Transcription with speaker diarization\n",
    "- **Step 3**: Translation using Meta SeamlessM4T v2\n",
    "- **Step 4**: Voice cloning with OpenVoice v2\n",
    "- **Step 5**: Intelligent audio-video synchronization\n",
    "- **Step 6**: Subtitle generation and integration\n",
    "- **Step 7**: Quality assurance and final assembly\n",
    "- **Step 8**: Batch processing with checkpointing\n",
    "\n",
    "### Requirements:\n",
    "- Kaggle GPU environment (P100/T4/V100)\n",
    "- Video files up to 8GB each\n",
    "- Arabic source language (Egyptian dialect supported)\n",
    "- Output: English and German dubbed videos with subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:51:56.199395Z",
     "iopub.status.busy": "2025-06-16T20:51:56.199101Z",
     "iopub.status.idle": "2025-06-16T20:51:56.212954Z",
     "shell.execute_reply": "2025-06-16T20:51:56.211826Z",
     "shell.execute_reply.started": "2025-06-16T20:51:56.199365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Running on Kaggle: False\n",
      "ðŸ’» Running in local environment\n",
      "   Note: For local use, consider using the individual Python files\n"
     ]
    }
   ],
   "source": [
    "# Check if we're running on Kaggle and setup environment\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "print(f\"ðŸŒ Running on Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(f\"ðŸ“ Working directory: /kaggle/working\")\n",
    "    print(f\"ðŸ“¥ Input directory: /kaggle/input\")\n",
    "    \n",
    "    # Check available GPU\n",
    "    print(\"\\nðŸ–¥ï¸  GPU Information:\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            for line in result.stdout.strip().split('\\n'):\n",
    "                if line.strip():\n",
    "                    gpu_name, memory = line.split(', ')\n",
    "                    print(f\"   ðŸš€ {gpu_name} ({memory}MB)\")\n",
    "        else:\n",
    "            print(\"   âŒ No GPU detected\")\n",
    "    except:\n",
    "        print(\"   â“ GPU status unknown\")\n",
    "else:\n",
    "    print(\"ðŸ’» Running in local environment\")\n",
    "    print(\"   Note: For local use, consider using the individual Python files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:51:56.630709Z",
     "iopub.status.busy": "2025-06-16T20:51:56.630350Z",
     "iopub.status.idle": "2025-06-16T20:54:52.566694Z",
     "shell.execute_reply": "2025-06-16T20:54:52.565588Z",
     "shell.execute_reply.started": "2025-06-16T20:51:56.630681Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ FIXED Video Dubbing Installer v2.3\n",
      "ðŸ“ Environment: Local\n",
      "ðŸ Python: 3.11\n",
      "ðŸ“‚ Working directory: /Users/omarnagy/Downloads/Video Dubbing\n",
      "ðŸ” Detecting GPU and CUDA...\n",
      "  ðŸ’» nvidia-smi not found - assuming CPU environment\n",
      "ðŸ§¹ Enhanced cleanup...\n",
      "ðŸ”§ Updating pip...\n",
      "$ python -m pip install --upgrade pip setuptools wheel\n",
      "âš ï¸  /bin/sh: python: command not found\n",
      "ðŸ“¦ Installing base dependencies...\n",
      "ðŸ”§ Installing numpy>=1.24.0,<2.0.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting numpy<2.0.0,>=1.24.0\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing packaging>=21.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting packaging>=21.0\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: packaging\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed packaging-25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing setuptools>=60.0.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting setuptools>=60.0.0\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 80.9.0\n",
      "    Uninstalling setuptools-80.9.0:\n",
      "      Successfully uninstalled setuptools-80.9.0\n",
      "Successfully installed setuptools-80.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing wheel>=0.38.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting wheel>=0.38.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wheel\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.45.1\n",
      "    Uninstalling wheel-0.45.1:\n",
      "      Successfully uninstalled wheel-0.45.1\n",
      "Successfully installed wheel-0.45.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ FIXED PyTorch Installation...\n",
      "  ðŸ’» Installing CPU-only PyTorch...\n",
      "  ðŸ“¦ Command: /opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cpu torch torchaudio\n",
      "  âœ… PyTorch installation completed\n",
      "  âœ… PyTorch import successful\n",
      "  ðŸ” PyTorch version: 2.7.1\n",
      "  ðŸ’» CUDA not available, using CPU\n",
      "ðŸ¤– Installing core ML packages...\n",
      "ðŸ”§ Installing transformers>=4.30.0,<4.50.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting transformers<4.50.0,>=4.30.0\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m707.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.49.0\n",
      "    Uninstalling transformers-4.49.0:\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "Successfully installed transformers-4.49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing tokenizers>=0.13.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting tokenizers>=0.13.0\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "Successfully installed tokenizers-0.21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing safetensors>=0.3.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting safetensors>=0.3.0\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m418.4/418.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "Successfully installed safetensors-0.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing accelerate>=0.20.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting accelerate>=0.20.0\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.7.0\n",
      "    Uninstalling accelerate-1.7.0:\n",
      "      Successfully uninstalled accelerate-1.7.0\n",
      "Successfully installed accelerate-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing openai-whisper>=20231117...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting openai-whisper>=20231117\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=0cef988b9aa9c8a636ec7c0f6edc5892eb441bedaa49a4c5cdcf408526bcbf9a\n",
      "  Stored in directory: /private/var/folders/gc/pdy55fd963ddhvfwd6f5mwmm0000gn/T/pip-ephem-wheel-cache-mz4ylbdy/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: openai-whisper\n",
      "  Attempting uninstall: openai-whisper\n",
      "    Found existing installation: openai-whisper 20240930\n",
      "    Uninstalling openai-whisper-20240930:\n",
      "      Successfully uninstalled openai-whisper-20240930\n",
      "Successfully installed openai-whisper-20240930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ Installing audio/video packages...\n",
      "ðŸ”§ Installing librosa>=0.10.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting librosa>=0.10.0\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: librosa\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.11.0\n",
      "    Uninstalling librosa-0.11.0:\n",
      "      Successfully uninstalled librosa-0.11.0\n",
      "Successfully installed librosa-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing soundfile>=0.12.1...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soundfile\n",
      "  Attempting uninstall: soundfile\n",
      "    Found existing installation: soundfile 0.13.1\n",
      "    Uninstalling soundfile-0.13.1:\n",
      "      Successfully uninstalled soundfile-0.13.1\n",
      "Successfully installed soundfile-0.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing moviepy==1.0.3...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting moviepy==1.0.3\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: moviepy\n",
      "  Building wheel for moviepy (setup.py): started\n",
      "  Building wheel for moviepy (setup.py): finished with status 'done'\n",
      "  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110797 sha256=2426062eedd4d247777c3906904b401f3fa2fe6aa2fc5e453c0ceb43bd56be5a\n",
      "  Stored in directory: /private/var/folders/gc/pdy55fd963ddhvfwd6f5mwmm0000gn/T/pip-ephem-wheel-cache-trs7_46s/wheels/83/b1/d9/119ef7c144b44d591ec0a9a140465133c23ea95d2a161184ba\n",
      "Successfully built moviepy\n",
      "Installing collected packages: moviepy\n",
      "  Attempting uninstall: moviepy\n",
      "    Found existing installation: moviepy 1.0.3\n",
      "    Uninstalling moviepy-1.0.3:\n",
      "      Successfully uninstalled moviepy-1.0.3\n",
      "Successfully installed moviepy-1.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing opencv-python-headless>=4.8.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting opencv-python-headless>=4.8.0\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.11.0.86\n",
      "    Uninstalling opencv-python-headless-4.11.0.86:\n",
      "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing ffmpeg-python>=0.2.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting ffmpeg-python>=0.2.0\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: ffmpeg-python\n",
      "  Attempting uninstall: ffmpeg-python\n",
      "    Found existing installation: ffmpeg-python 0.2.0\n",
      "    Uninstalling ffmpeg-python-0.2.0:\n",
      "      Successfully uninstalled ffmpeg-python-0.2.0\n",
      "Successfully installed ffmpeg-python-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing utility packages...\n",
      "ðŸ”§ Installing tqdm>=4.65.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting tqdm>=4.65.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing requests>=2.31.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting requests>=2.31.0\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "Successfully installed requests-2.32.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing pandas>=1.5.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting pandas>=1.5.0\n",
      "  Downloading pandas-2.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m985.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.0-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.0\n",
      "    Uninstalling pandas-2.3.0:\n",
      "      Successfully uninstalled pandas-2.3.0\n",
      "Successfully installed pandas-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing numpy>=1.24.0,<2.0.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting numpy<2.0.0,>=1.24.0\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing scipy>=1.10.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting scipy>=1.10.0\n",
      "  Downloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.3\n",
      "    Uninstalling scipy-1.15.3:\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "Successfully installed scipy-1.15.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing matplotlib>=3.7.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting matplotlib>=3.7.0\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.3\n",
      "    Uninstalling matplotlib-3.10.3:\n",
      "      Successfully uninstalled matplotlib-3.10.3\n",
      "Successfully installed matplotlib-3.10.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing psutil>=5.9.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting psutil>=5.9.0\n",
      "  Downloading psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Downloading psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psutil\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 7.0.0\n",
      "    Uninstalling psutil-7.0.0:\n",
      "      Successfully uninstalled psutil-7.0.0\n",
      "Successfully installed psutil-7.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing optional packages...\n",
      "ðŸ”§ Installing speechbrain>=0.5.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting speechbrain>=0.5.0\n",
      "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: speechbrain\n",
      "  Attempting uninstall: speechbrain\n",
      "    Found existing installation: speechbrain 1.0.3\n",
      "    Uninstalling speechbrain-1.0.3:\n",
      "      Successfully uninstalled speechbrain-1.0.3\n",
      "Successfully installed speechbrain-1.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing dtw-python>=1.3.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting dtw-python>=1.3.0\n",
      "  Downloading dtw_python-1.5.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dtw_python-1.5.3-cp311-cp311-macosx_11_0_arm64.whl (376 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.6/376.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dtw-python\n",
      "  Attempting uninstall: dtw-python\n",
      "    Found existing installation: dtw-python 1.5.3\n",
      "    Uninstalling dtw-python-1.5.3:\n",
      "      Successfully uninstalled dtw-python-1.5.3\n",
      "Successfully installed dtw-python-1.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing noisereduce>=3.0.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting noisereduce>=3.0.0\n",
      "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: noisereduce\n",
      "  Attempting uninstall: noisereduce\n",
      "    Found existing installation: noisereduce 3.0.3\n",
      "    Uninstalling noisereduce-3.0.3:\n",
      "      Successfully uninstalled noisereduce-3.0.3\n",
      "Successfully installed noisereduce-3.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing hyperpyyaml>=1.2.0...\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  âš ï¸  Installation reported success but package not found\n",
      "  ðŸ”„ Trying force reinstall...\n",
      "Collecting hyperpyyaml>=1.2.0\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: hyperpyyaml\n",
      "  Attempting uninstall: hyperpyyaml\n",
      "    Found existing installation: HyperPyYAML 1.2.2\n",
      "    Uninstalling HyperPyYAML-1.2.2:\n",
      "      Successfully uninstalled HyperPyYAML-1.2.2\n",
      "Successfully installed hyperpyyaml-1.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Comprehensive Testing...\n",
      "ðŸ” Testing critical imports...\n",
      "  âœ… torch v2.7.1 (CUDA: False, Devices: 0)\n",
      "  âœ… whisper v20240930 (14 models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… transformers v4.49.0\n",
      "  âœ… librosa v0.11.0\n",
      "  âœ… cv2 v4.11.0\n",
      "  âœ… moviepy v1.0.3\n",
      "  âœ… soundfile v0.13.1\n",
      "ðŸ”¬ Testing functionality...\n",
      "  âœ… PyTorch tensor operations (GPU: False)\n",
      "  âœ… Whisper model loading\n",
      "  âœ… Audio processing\n",
      "\n",
      "ðŸ“Š INSTALLATION SUMMARY\n",
      "======================================================================\n",
      "ðŸ§ª Critical imports: 7/7 (100.0%)\n",
      "ðŸ¤– ML packages: 0/5\n",
      "ðŸŽµ AV packages: 0/5\n",
      "ðŸ”§ Utilities: 0/7\n",
      "ðŸ“¦ Optional: 0/4\n",
      "\n",
      "ðŸŽ¯ SYSTEM STATUS:\n",
      "  ðŸ’» GPU Acceleration: DISABLED (CPU mode)\n",
      "  âœ… ML Pipeline: READY\n",
      "  âœ… AV Processing: READY\n",
      "\n",
      "ðŸŽ‰ INSTALLATION SUCCESSFUL!\n",
      "ðŸš€ Ready for video dubbing pipeline!\n",
      "ðŸ’¡ All systems operational\n",
      "\n",
      "ðŸ“‹ TROUBLESHOOTING TIPS:\n",
      "  1. If PyTorch failed: Restart kernel and try again\n",
      "  2. If imports fail: Check Python path and permissions\n",
      "  3. If CUDA issues: Verify GPU is available in Kaggle settings\n",
      "  4. For persistent issues: Switch to CPU-only mode\n",
      "\n",
      "â° Installation completed: 15:06:06\n",
      "ðŸ”„ Kernel restart recommended for best results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1000\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  FIXED KAGGLE INSTALLER v2.3 â€” Video Dubbing Pipeline (PyTorch Installation Fixed)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  âœ… Fixed PyTorch installation â€¢ âœ… Better error handling â€¢ âœ… Kaggle optimization\n",
    "#  âœ… Step-by-step debugging â€¢ âœ… Fallback strategies â€¢ âœ… 2025 compatibility\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import subprocess, sys, importlib, pathlib, re, types, os, time, shutil\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Environment detection\n",
    "IS_KAGGLE = any(\"/kaggle\" in p for p in sys.path) or os.path.exists('/kaggle')\n",
    "PYTHON_VERSION = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "\n",
    "print(f\"ðŸŽ¬ FIXED Video Dubbing Installer v2.3\")\n",
    "print(f\"ðŸ“ Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"ðŸ Python: {PYTHON_VERSION}\")\n",
    "print(f\"ðŸ“‚ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Enhanced helper functions\n",
    "def sh(cmd, check=True, timeout=300, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"$ {cmd}\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=check, \n",
    "                              capture_output=True, text=True, timeout=timeout)\n",
    "        if verbose and result.stdout:\n",
    "            print(f\"   {result.stdout.strip()}\")\n",
    "        if result.stderr and \"warning\" not in result.stderr.lower():\n",
    "            if verbose:\n",
    "                print(f\"âš ï¸  {result.stderr.strip()}\")\n",
    "        return result\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"â±ï¸  Command timed out after {timeout}s\")\n",
    "        return None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Command failed with code {e.returncode}\")\n",
    "        if e.stdout:\n",
    "            print(f\"   stdout: {e.stdout.strip()}\")\n",
    "        if e.stderr:\n",
    "            print(f\"   stderr: {e.stderr.strip()}\")\n",
    "        if check:\n",
    "            raise\n",
    "        return e\n",
    "\n",
    "def check_pip_install_success(package_name):\n",
    "    \"\"\"Verify if a package was actually installed\"\"\"\n",
    "    try:\n",
    "        result = sh(f\"python -m pip show {package_name}\", check=False, verbose=False)\n",
    "        return result and result.returncode == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def force_pip_install(package, max_retries=3, use_cache=False):\n",
    "    \"\"\"Force install a package with multiple strategies\"\"\"\n",
    "    print(f\"ðŸ”§ Installing {package}...\")\n",
    "    \n",
    "    # Base flags\n",
    "    flags = []\n",
    "    if IS_KAGGLE:\n",
    "        flags.extend([\"--user\", \"--no-warn-script-location\"])\n",
    "    \n",
    "    if not use_cache:\n",
    "        flags.append(\"--no-cache-dir\")\n",
    "    \n",
    "    # Strategy 1: Normal install\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + flags + [package]\n",
    "            result = subprocess.run(cmd, check=True, timeout=300, \n",
    "                                  capture_output=True, text=True)\n",
    "            \n",
    "            # Verify installation\n",
    "            pkg_name = re.split(r\"[<>=!]\", package)[0]\n",
    "            if check_pip_install_success(pkg_name):\n",
    "                print(f\"  âœ… {package} installed successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"  âš ï¸  Installation reported success but package not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Attempt {attempt + 1} failed: {str(e)[:100]}...\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    # Strategy 2: Force reinstall\n",
    "    print(f\"  ðŸ”„ Trying force reinstall...\")\n",
    "    try:\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"--no-deps\"] + flags + [package]\n",
    "        subprocess.run(cmd, check=True, timeout=300)\n",
    "        \n",
    "        pkg_name = re.split(r\"[<>=!]\", package)[0]\n",
    "        if check_pip_install_success(pkg_name):\n",
    "            print(f\"  âœ… {package} force installed successfully\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Force install failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def detect_gpu_and_cuda():\n",
    "    \"\"\"Enhanced GPU and CUDA detection\"\"\"\n",
    "    gpu_info = {\"has_gpu\": False, \"cuda_version\": None, \"gpu_name\": None, \"cuda_major\": None}\n",
    "    \n",
    "    print(\"ðŸ” Detecting GPU and CUDA...\")\n",
    "    \n",
    "    try:\n",
    "        # Check for nvidia-smi\n",
    "        result = sh(\"which nvidia-smi\", check=False, verbose=False)\n",
    "        if not result or result.returncode != 0:\n",
    "            print(\"  ðŸ’» nvidia-smi not found - assuming CPU environment\")\n",
    "            return gpu_info\n",
    "            \n",
    "        # Check for GPU\n",
    "        result = sh(\"nvidia-smi --query-gpu=name --format=csv,noheader\", check=False, verbose=False)\n",
    "        if result and result.returncode == 0:\n",
    "            gpu_info[\"has_gpu\"] = True\n",
    "            gpu_info[\"gpu_name\"] = result.stdout.strip().split('\\n')[0]\n",
    "            print(f\"  ðŸ–¥ï¸  GPU found: {gpu_info['gpu_name']}\")\n",
    "            \n",
    "            # Check CUDA version\n",
    "            cuda_result = sh(\"nvcc --version\", check=False, verbose=False)\n",
    "            if cuda_result and cuda_result.returncode == 0:\n",
    "                match = re.search(r'release (\\d+)\\.(\\d+)', cuda_result.stdout)\n",
    "                if match:\n",
    "                    major, minor = match.groups()\n",
    "                    gpu_info[\"cuda_version\"] = f\"{major}.{minor}\"\n",
    "                    gpu_info[\"cuda_major\"] = int(major)\n",
    "                    print(f\"  ðŸ” CUDA version: {gpu_info['cuda_version']}\")\n",
    "            else:\n",
    "                print(\"  âš ï¸  nvcc not found - CUDA may not be properly installed\")\n",
    "        else:\n",
    "            print(\"  ðŸ’» No GPU detected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  GPU detection error: {e}\")\n",
    "    \n",
    "    return gpu_info\n",
    "\n",
    "# Detect environment\n",
    "gpu_info = detect_gpu_and_cuda()\n",
    "\n",
    "# System dependencies for Kaggle\n",
    "if IS_KAGGLE:\n",
    "    print(\"ðŸ“¦ Installing system dependencies...\")\n",
    "    sh(\"apt-get -qq update\", check=False)\n",
    "    sh(\"apt-get -qq install -y ffmpeg git libsndfile1-dev portaudio19-dev\", check=False)\n",
    "\n",
    "# Enhanced cleanup\n",
    "print(\"ðŸ§¹ Enhanced cleanup...\")\n",
    "CLEANUP_PACKAGES = [\n",
    "    \"torch\", \"torchaudio\", \"torchvision\", \"torch-audio\", \"torch-vision\",\n",
    "    \"speechbrain\", \"whisper\", \"openai-whisper\", \"dtw\", \"dtw-python\", \n",
    "    \"noisereduce\", \"hyperpyyaml\", \"ruamel.yaml\"\n",
    "]\n",
    "\n",
    "for pkg in CLEANUP_PACKAGES:\n",
    "    sh(f\"python -m pip uninstall -y -q {pkg}\", check=False, verbose=False)\n",
    "\n",
    "# Clear pip cache\n",
    "sh(\"python -m pip cache purge\", check=False, verbose=False)\n",
    "\n",
    "# Update pip itself\n",
    "print(\"ðŸ”§ Updating pip...\")\n",
    "sh(\"python -m pip install --upgrade pip setuptools wheel\", check=False)\n",
    "\n",
    "# Install base dependencies first\n",
    "print(\"ðŸ“¦ Installing base dependencies...\")\n",
    "BASE_DEPS = [\n",
    "    \"numpy>=1.24.0,<2.0.0\",\n",
    "    \"packaging>=21.0\",\n",
    "    \"setuptools>=60.0.0\",\n",
    "    \"wheel>=0.38.0\",\n",
    "]\n",
    "\n",
    "for dep in BASE_DEPS:\n",
    "    force_pip_install(dep)\n",
    "\n",
    "# FIXED PyTorch installation\n",
    "print(\"ðŸ”¥ FIXED PyTorch Installation...\")\n",
    "\n",
    "def install_pytorch_fixed():\n",
    "    \"\"\"Fixed PyTorch installation with proper error handling\"\"\"\n",
    "    \n",
    "    # Determine the right PyTorch version and index\n",
    "    if not gpu_info[\"has_gpu\"]:\n",
    "        print(\"  ðŸ’» Installing CPU-only PyTorch...\")\n",
    "        index_url = \"https://download.pytorch.org/whl/cpu\"\n",
    "        torch_version = \"torch torchaudio\"\n",
    "    else:\n",
    "        cuda_major = gpu_info.get(\"cuda_major\", 11)\n",
    "        print(f\"  ðŸš€ Installing PyTorch for CUDA {cuda_major}.x...\")\n",
    "        \n",
    "        if cuda_major >= 12:\n",
    "            index_url = \"https://download.pytorch.org/whl/cu121\"  # Use cu121 for broad compatibility\n",
    "            torch_version = \"torch torchaudio\"\n",
    "        else:\n",
    "            index_url = \"https://download.pytorch.org/whl/cu118\"\n",
    "            torch_version = \"torch torchaudio\"\n",
    "    \n",
    "    # Install PyTorch with proper flags\n",
    "    cmd_parts = [\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \n",
    "        \"--no-cache-dir\", \"--index-url\", index_url\n",
    "    ]\n",
    "    \n",
    "    if IS_KAGGLE:\n",
    "        cmd_parts.extend([\"--user\", \"--no-warn-script-location\"])\n",
    "    \n",
    "    cmd_parts.extend(torch_version.split())\n",
    "    \n",
    "    print(f\"  ðŸ“¦ Command: {' '.join(cmd_parts)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd_parts, check=True, timeout=600, \n",
    "                              capture_output=True, text=True)\n",
    "        print(\"  âœ… PyTorch installation completed\")\n",
    "        \n",
    "        # Verify installation\n",
    "        time.sleep(2)  # Give time for installation to settle\n",
    "        \n",
    "        # Test import\n",
    "        try:\n",
    "            import torch\n",
    "            print(f\"  âœ… PyTorch import successful\")\n",
    "            print(f\"  ðŸ” PyTorch version: {torch.__version__}\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"  ðŸš€ CUDA available: {torch.cuda.device_count()} devices\")\n",
    "                print(f\"  ðŸŽ¯ Current device: {torch.cuda.get_device_name(0)}\")\n",
    "            else:\n",
    "                print(f\"  ðŸ’» CUDA not available, using CPU\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except ImportError as e:\n",
    "            print(f\"  âŒ PyTorch import failed: {e}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"  â±ï¸  PyTorch installation timed out\")\n",
    "        return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  âŒ PyTorch installation failed: {e}\")\n",
    "        if e.stdout:\n",
    "            print(f\"     stdout: {e.stdout[-200:]}\")  # Last 200 chars\n",
    "        if e.stderr:\n",
    "            print(f\"     stderr: {e.stderr[-200:]}\")  # Last 200 chars\n",
    "        return False\n",
    "\n",
    "# Attempt PyTorch installation\n",
    "pytorch_success = install_pytorch_fixed()\n",
    "\n",
    "# Fallback to CPU if GPU installation failed\n",
    "if not pytorch_success and gpu_info[\"has_gpu\"]:\n",
    "    print(\"ðŸ”„ GPU PyTorch failed, trying CPU version...\")\n",
    "    gpu_info[\"has_gpu\"] = False  # Force CPU installation\n",
    "    pytorch_success = install_pytorch_fixed()\n",
    "\n",
    "if not pytorch_success:\n",
    "    print(\"ðŸš¨ Critical: PyTorch installation completely failed!\")\n",
    "    print(\"ðŸ”§ Manual fix needed - try restarting kernel and running again\")\n",
    "\n",
    "# Core ML packages\n",
    "print(\"ðŸ¤– Installing core ML packages...\")\n",
    "CORE_ML = [\n",
    "    \"transformers>=4.30.0,<4.50.0\",\n",
    "    \"tokenizers>=0.13.0\",\n",
    "    \"safetensors>=0.3.0\",\n",
    "    \"accelerate>=0.20.0\",\n",
    "    \"openai-whisper>=20231117\",\n",
    "]\n",
    "\n",
    "ml_success = 0\n",
    "for package in CORE_ML:\n",
    "    if force_pip_install(package):\n",
    "        ml_success += 1\n",
    "\n",
    "# Audio/Video processing packages\n",
    "print(\"ðŸŽµ Installing audio/video packages...\")\n",
    "AV_PACKAGES = [\n",
    "    \"librosa>=0.10.0\",\n",
    "    \"soundfile>=0.12.1\",\n",
    "    \"moviepy==1.0.3\",\n",
    "    \"opencv-python-headless>=4.8.0\",\n",
    "    \"ffmpeg-python>=0.2.0\",\n",
    "]\n",
    "\n",
    "av_success = 0\n",
    "for package in AV_PACKAGES:\n",
    "    if force_pip_install(package):\n",
    "        av_success += 1\n",
    "\n",
    "# Utility packages\n",
    "print(\"ðŸ”§ Installing utility packages...\")\n",
    "UTILITIES = [\n",
    "    \"tqdm>=4.65.0\",\n",
    "    \"requests>=2.31.0\",\n",
    "    \"pandas>=1.5.0\",\n",
    "    \"numpy>=1.24.0,<2.0.0\",\n",
    "    \"scipy>=1.10.0\",\n",
    "    \"matplotlib>=3.7.0\",\n",
    "    \"psutil>=5.9.0\",\n",
    "]\n",
    "\n",
    "util_success = 0\n",
    "for package in UTILITIES:\n",
    "    if force_pip_install(package):\n",
    "        util_success += 1\n",
    "\n",
    "# Optional packages\n",
    "print(\"ðŸ”§ Installing optional packages...\")\n",
    "OPTIONAL = [\n",
    "    \"speechbrain>=0.5.0\",\n",
    "    \"dtw-python>=1.3.0\",\n",
    "    \"noisereduce>=3.0.0\",\n",
    "    \"hyperpyyaml>=1.2.0\",\n",
    "]\n",
    "\n",
    "optional_success = 0\n",
    "for package in OPTIONAL:\n",
    "    if force_pip_install(package):\n",
    "        optional_success += 1\n",
    "\n",
    "# Comprehensive testing\n",
    "print(\"ðŸ§ª Comprehensive Testing...\")\n",
    "\n",
    "def test_import_with_info(module_name, import_name=None, test_func=None):\n",
    "    \"\"\"Test import with detailed information\"\"\"\n",
    "    try:\n",
    "        if import_name:\n",
    "            module = importlib.import_module(import_name)\n",
    "        else:\n",
    "            module = importlib.import_module(module_name)\n",
    "        \n",
    "        info = \"\"\n",
    "        if hasattr(module, '__version__'):\n",
    "            info = f\" v{module.__version__}\"\n",
    "        \n",
    "        if test_func:\n",
    "            test_result = test_func(module)\n",
    "            if test_result:\n",
    "                info += f\" ({test_result})\"\n",
    "        \n",
    "        print(f\"  âœ… {module_name}{info}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)[:50] + \"...\" if len(str(e)) > 50 else str(e)\n",
    "        print(f\"  âŒ {module_name}: {error_msg}\")\n",
    "        return False\n",
    "\n",
    "# Test critical imports\n",
    "print(\"ðŸ” Testing critical imports...\")\n",
    "test_results = {}\n",
    "\n",
    "# PyTorch\n",
    "test_results[\"torch\"] = test_import_with_info(\"torch\", test_func=lambda m: \n",
    "    f\"CUDA: {m.cuda.is_available()}, Devices: {m.cuda.device_count()}\" if hasattr(m, 'cuda') else \"CPU only\")\n",
    "\n",
    "# Whisper\n",
    "test_results[\"whisper\"] = test_import_with_info(\"whisper\", test_func=lambda m: \n",
    "    f\"{len(m.available_models())} models\" if hasattr(m, 'available_models') else None)\n",
    "\n",
    "# Other critical packages\n",
    "critical_packages = [\n",
    "    (\"transformers\", \"transformers\"),\n",
    "    (\"librosa\", \"librosa\"),\n",
    "    (\"cv2\", \"cv2\"),\n",
    "    (\"moviepy\", \"moviepy\"),\n",
    "    (\"soundfile\", \"soundfile\"),\n",
    "]\n",
    "\n",
    "for display_name, import_name in critical_packages:\n",
    "    test_results[display_name] = test_import_with_info(display_name, import_name)\n",
    "\n",
    "# Functionality tests\n",
    "print(\"ðŸ”¬ Testing functionality...\")\n",
    "\n",
    "# Test PyTorch operations\n",
    "try:\n",
    "    import torch\n",
    "    x = torch.randn(3, 3)\n",
    "    y = torch.matmul(x, x)\n",
    "    if torch.cuda.is_available():\n",
    "        x_gpu = x.cuda()\n",
    "        y_gpu = torch.matmul(x_gpu, x_gpu)\n",
    "        functionality_test_gpu = True\n",
    "    else:\n",
    "        functionality_test_gpu = False\n",
    "    print(f\"  âœ… PyTorch tensor operations (GPU: {functionality_test_gpu})\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ PyTorch operations: {e}\")\n",
    "\n",
    "# Test Whisper\n",
    "try:\n",
    "    import whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "    print(\"  âœ… Whisper model loading\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Whisper model loading: {e}\")\n",
    "\n",
    "# Test audio processing\n",
    "try:\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "    dummy_audio = np.random.randn(1000)\n",
    "    mfcc = librosa.feature.mfcc(y=dummy_audio, sr=22050)\n",
    "    print(\"  âœ… Audio processing\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Audio processing: {e}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸ“Š INSTALLATION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "passed = sum(test_results.values())\n",
    "total = len(test_results)\n",
    "success_rate = (passed / total) * 100\n",
    "\n",
    "print(f\"ðŸ§ª Critical imports: {passed}/{total} ({success_rate:.1f}%)\")\n",
    "print(f\"ðŸ¤– ML packages: {ml_success}/{len(CORE_ML)}\")\n",
    "print(f\"ðŸŽµ AV packages: {av_success}/{len(AV_PACKAGES)}\")\n",
    "print(f\"ðŸ”§ Utilities: {util_success}/{len(UTILITIES)}\")\n",
    "print(f\"ðŸ“¦ Optional: {optional_success}/{len(OPTIONAL)}\")\n",
    "\n",
    "# System status\n",
    "print(f\"\\nðŸŽ¯ SYSTEM STATUS:\")\n",
    "if pytorch_success and test_results.get(\"torch\", False):\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  ðŸš€ GPU Acceleration: ENABLED\")\n",
    "            print(f\"     Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"     Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        else:\n",
    "            print(f\"  ðŸ’» GPU Acceleration: DISABLED (CPU mode)\")\n",
    "    except:\n",
    "        print(f\"  âš ï¸  PyTorch status unclear\")\n",
    "else:\n",
    "    print(f\"  âŒ PyTorch: FAILED\")\n",
    "\n",
    "# Pipeline readiness\n",
    "core_ready = all(test_results.get(pkg, False) for pkg in [\"torch\", \"whisper\", \"transformers\"])\n",
    "av_ready = all(test_results.get(pkg, False) for pkg in [\"librosa\", \"cv2\", \"moviepy\"])\n",
    "\n",
    "if core_ready:\n",
    "    print(f\"  âœ… ML Pipeline: READY\")\n",
    "else:\n",
    "    print(f\"  âŒ ML Pipeline: INCOMPLETE\")\n",
    "\n",
    "if av_ready:\n",
    "    print(f\"  âœ… AV Processing: READY\")\n",
    "else:\n",
    "    print(f\"  âŒ AV Processing: INCOMPLETE\")\n",
    "\n",
    "# Final verdict\n",
    "if pytorch_success and core_ready and av_ready:\n",
    "    print(f\"\\nðŸŽ‰ INSTALLATION SUCCESSFUL!\")\n",
    "    print(f\"ðŸš€ Ready for video dubbing pipeline!\")\n",
    "    print(f\"ðŸ’¡ All systems operational\")\n",
    "elif pytorch_success and core_ready:\n",
    "    print(f\"\\nâš ï¸  MOSTLY SUCCESSFUL\")\n",
    "    print(f\"ðŸ”§ Core ML working, some AV issues\")\n",
    "    print(f\"ðŸ’¡ Should work with basic functionality\")\n",
    "else:\n",
    "    print(f\"\\nâŒ CRITICAL ISSUES DETECTED\")\n",
    "    print(f\"ðŸ”§ PyTorch or core ML components failed\")\n",
    "    print(f\"ðŸ’¡ Kernel restart recommended\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ TROUBLESHOOTING TIPS:\")\n",
    "print(f\"  1. If PyTorch failed: Restart kernel and try again\")\n",
    "print(f\"  2. If imports fail: Check Python path and permissions\")\n",
    "print(f\"  3. If CUDA issues: Verify GPU is available in Kaggle settings\")\n",
    "print(f\"  4. For persistent issues: Switch to CPU-only mode\")\n",
    "\n",
    "print(f\"\\nâ° Installation completed: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"ðŸ”„ Kernel restart recommended for best results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:59:34.343285Z",
     "iopub.status.busy": "2025-06-16T20:59:34.341604Z",
     "iopub.status.idle": "2025-06-16T20:59:34.357950Z",
     "shell.execute_reply": "2025-06-16T20:59:34.356997Z",
     "shell.execute_reply.started": "2025-06-16T20:59:34.343245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Configuring Kaggle Environment\n",
      "========================================\n",
      "ðŸ’» Local environment - no Kaggle-specific setup needed\n",
      "\n",
      "ðŸ–¥ï¸  GPU and Memory Configuration:\n",
      "âš ï¸  No GPU available - will use CPU (much slower)\n",
      "\n",
      "ðŸŽ¯ Environment ready for video dubbing pipeline!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ› ï¸ Kaggle Environment Setup and Path Configuration\n",
    "print(\"ðŸ”§ Configuring Kaggle Environment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Ensure user-installed packages are in path\n",
    "    import site\n",
    "    import sys\n",
    "    \n",
    "    # Add user site-packages to Python path\n",
    "    user_site = site.getusersitepackages()\n",
    "    if user_site not in sys.path:\n",
    "        sys.path.insert(0, user_site)\n",
    "        print(f\"âœ… Added user site-packages to path: {user_site}\")\n",
    "    \n",
    "    # Also add common Kaggle user install locations\n",
    "    common_paths = [\n",
    "        \"/root/.local/lib/python3.10/site-packages\",\n",
    "        \"/home/.local/lib/python3.10/site-packages\",\n",
    "        \"/opt/conda/lib/python3.10/site-packages\"\n",
    "    ]\n",
    "    \n",
    "    for path in common_paths:\n",
    "        if os.path.exists(path) and path not in sys.path:\n",
    "            sys.path.insert(0, path)\n",
    "            print(f\"âœ… Added path: {path}\")\n",
    "    \n",
    "    # Refresh importlib cache\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    \n",
    "    # Set environment variables for better package detection\n",
    "    os.environ['PYTHONPATH'] = ':'.join(sys.path)\n",
    "    \n",
    "    print(f\"ðŸ” Current Python paths:\")\n",
    "    for i, path in enumerate(sys.path[:5]):  # Show first 5 paths\n",
    "        print(f\"   {i+1}. {path}\")\n",
    "    if len(sys.path) > 5:\n",
    "        print(f\"   ... and {len(sys.path)-5} more paths\")\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ’» Local environment - no Kaggle-specific setup needed\")\n",
    "\n",
    "# Memory and GPU setup\n",
    "print(f\"\\nðŸ–¥ï¸  GPU and Memory Configuration:\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        \n",
    "        print(f\"âœ… GPU Available: {device_name}\")\n",
    "        print(f\"   Device count: {device_count}\")\n",
    "        print(f\"   Current device: {current_device}\")\n",
    "        \n",
    "        # Clear any existing GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Get memory info\n",
    "        memory_allocated = torch.cuda.memory_allocated(current_device) / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved(current_device) / 1024**3\n",
    "        \n",
    "        print(f\"   Memory allocated: {memory_allocated:.2f} GB\")\n",
    "        print(f\"   Memory reserved: {memory_reserved:.2f} GB\")\n",
    "        \n",
    "        # Set memory fraction to prevent OOM\n",
    "        if not hasattr(torch.cuda, '_initialized') or not torch.cuda._initialized:\n",
    "            torch.cuda.set_per_process_memory_fraction(0.9)  # Use 90% of GPU memory\n",
    "            print(f\"   Set memory fraction to 90%\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  No GPU available - will use CPU (much slower)\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch not available\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Environment ready for video dubbing pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:59:40.236308Z",
     "iopub.status.busy": "2025-06-16T20:59:40.235954Z",
     "iopub.status.idle": "2025-06-16T20:59:40.245051Z",
     "shell.execute_reply": "2025-06-16T20:59:40.243848Z",
     "shell.execute_reply.started": "2025-06-16T20:59:40.236286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/omarnagy/Downloads/Video Dubbing/working\n",
      "âœ“ Created directory: models/\n",
      "âœ“ Created directory: temp/\n",
      "âœ“ Created directory: output/\n",
      "âœ“ Created directory: logs/\n",
      "âœ“ Created directory: checkpoints/\n",
      "âœ“ Created directory: scripts/\n",
      "\n",
      "âœ… Directory structure ready!\n"
     ]
    }
   ],
   "source": [
    "# Create project files in working directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory\n",
    "if IS_KAGGLE:\n",
    "    os.chdir('/kaggle/working')\n",
    "else:\n",
    "    # Create local working directory\n",
    "    Path('./working').mkdir(exist_ok=True)\n",
    "    os.chdir('./working')\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Create necessary directories\n",
    "directories = ['models', 'temp', 'output', 'logs', 'checkpoints', 'scripts']\n",
    "\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(exist_ok=True)\n",
    "    print(f\"âœ“ Created directory: {directory}/\")\n",
    "\n",
    "print(\"\\nâœ… Directory structure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Initialize Dubbing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:59:42.943425Z",
     "iopub.status.busy": "2025-06-16T20:59:42.943087Z",
     "iopub.status.idle": "2025-06-16T20:59:42.951617Z",
     "shell.execute_reply": "2025-06-16T20:59:42.950633Z",
     "shell.execute_reply.started": "2025-06-16T20:59:42.943402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration file created\n"
     ]
    }
   ],
   "source": [
    "# Write the main configuration file\n",
    "config_code = '''\n",
    "\"\"\"Enhanced Video Dubbing Configuration\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, local_mode=False):\n",
    "        self.local_mode = local_mode\n",
    "        self.setup_directories()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        if self.local_mode or not os.path.exists(\"/kaggle\"):\n",
    "            self.WORKING_DIR = Path(\"./working\")\n",
    "            self.INPUT_DIR = Path(\"./input\")\n",
    "        else:\n",
    "            self.WORKING_DIR = Path(\"/kaggle/working\")\n",
    "            self.INPUT_DIR = Path(\"/kaggle/input\")\n",
    "        \n",
    "        self.MODELS_DIR = self.WORKING_DIR / \"models\"\n",
    "        self.TEMP_DIR = self.WORKING_DIR / \"temp\"\n",
    "        self.OUTPUT_DIR = self.WORKING_DIR / \"output\"\n",
    "        self.LOGS_DIR = self.WORKING_DIR / \"logs\"\n",
    "        self.CHECKPOINTS_DIR = self.WORKING_DIR / \"checkpoints\"\n",
    "        \n",
    "        for directory in [self.MODELS_DIR, self.TEMP_DIR, self.OUTPUT_DIR, \n",
    "                         self.LOGS_DIR, self.CHECKPOINTS_DIR]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Model Configuration\n",
    "    WHISPER_MODEL = \"large-v3\"\n",
    "    SEAMLESS_MODEL = \"facebook/hf-seamless-m4t-large\"\n",
    "    \n",
    "    # Language Settings\n",
    "    SOURCE_LANGUAGE = \"ar\"\n",
    "    TARGET_LANGUAGES = [\"en\", \"de\"]\n",
    "    \n",
    "    # Processing Settings\n",
    "    AUDIO_SAMPLE_RATE = 48000\n",
    "    GPU_MEMORY_FRACTION = 0.8\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_CHUNK_LENGTH = 30.0\n",
    "    \n",
    "    # Quality Settings\n",
    "    NOISE_REDUCTION_STRENGTH = 0.5\n",
    "    VOICE_SIMILARITY_THRESHOLD = 0.85\n",
    "    \n",
    "    # File Processing\n",
    "    MAX_FILE_SIZE_GB = 8\n",
    "    SUPPORTED_VIDEO_FORMATS = [\".mp4\", \".avi\", \".mkv\", \".mov\"]\n",
    "    \n",
    "    # Error Handling\n",
    "    MAX_RETRIES = 3\n",
    "    RETRY_DELAY = 60\n",
    "    \n",
    "    def get_video_output_path(self, video_name, language, resolution=\"1080p\"):\n",
    "        return self.OUTPUT_DIR / video_name / f\"{video_name}_{language}_{resolution}.mp4\"\n",
    "    \n",
    "    def get_log_path(self, video_name):\n",
    "        return self.LOGS_DIR / f\"{video_name}_processing.log\"\n",
    "\n",
    "config = Config(local_mode=not os.path.exists(\"/kaggle\"))\n",
    "'''\n",
    "\n",
    "with open('config.py', 'w') as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(\"âœ“ Configuration file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:59:45.383279Z",
     "iopub.status.busy": "2025-06-16T20:59:45.382874Z",
     "iopub.status.idle": "2025-06-16T20:59:45.394010Z",
     "shell.execute_reply": "2025-06-16T20:59:45.392744Z",
     "shell.execute_reply.started": "2025-06-16T20:59:45.383247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:07:54,863 - WARNING - No GPU available - will use CPU (slower processing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment initialized\n"
     ]
    }
   ],
   "source": [
    "# Import our configuration\n",
    "from config import config\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(config.LOGS_DIR / 'pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    logger.info(f\"GPU available: {device} ({memory:.1f}GB)\")\n",
    "else:\n",
    "    logger.warning(\"No GPU available - will use CPU (slower processing)\")\n",
    "\n",
    "print(\"âœ“ Environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Video Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T20:59:47.292747Z",
     "iopub.status.busy": "2025-06-16T20:59:47.291889Z",
     "iopub.status.idle": "2025-06-16T20:59:47.321954Z",
     "shell.execute_reply": "2025-06-16T20:59:47.320862Z",
     "shell.execute_reply.started": "2025-06-16T20:59:47.292709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Found 0 valid video files\n",
      "\n",
      "âš ï¸  No video files found!\n",
      "Please ensure your video files are uploaded to the Kaggle dataset or input directory.\n",
      "Supported formats: ['.mp4', '.avi', '.mkv', '.mov']\n"
     ]
    }
   ],
   "source": [
    "# Discover available video files\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def discover_videos():\n",
    "    \"\"\"Find video files in input directory\"\"\"\n",
    "    video_files = []\n",
    "    \n",
    "    # Search in Kaggle input directory\n",
    "    search_paths = []\n",
    "    if IS_KAGGLE:\n",
    "        # Search all subdirectories in /kaggle/input\n",
    "        input_dirs = list(Path('/kaggle/input').glob('*'))\n",
    "        for input_dir in input_dirs:\n",
    "            if input_dir.is_dir():\n",
    "                search_paths.append(input_dir)\n",
    "    else:\n",
    "        # Local input directory\n",
    "        search_paths = [Path('./input')]\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        if search_path.exists():\n",
    "            for ext in config.SUPPORTED_VIDEO_FORMATS:\n",
    "                pattern = str(search_path / f'**/*{ext}')\n",
    "                found_files = glob.glob(pattern, recursive=True)\n",
    "                video_files.extend([Path(f) for f in found_files])\n",
    "    \n",
    "    # Filter by file size\n",
    "    valid_videos = []\n",
    "    for video_file in video_files:\n",
    "        try:\n",
    "            file_size_gb = video_file.stat().st_size / (1024**3)\n",
    "            if file_size_gb <= config.MAX_FILE_SIZE_GB:\n",
    "                valid_videos.append(video_file)\n",
    "                print(f\"Found: {video_file.name} ({file_size_gb:.1f}GB)\")\n",
    "            else:\n",
    "                print(f\"Skipping oversized: {video_file.name} ({file_size_gb:.1f}GB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking {video_file}: {e}\")\n",
    "    \n",
    "    return valid_videos\n",
    "\n",
    "# Discover videos\n",
    "video_files = discover_videos()\n",
    "print(f\"\\nâœ“ Found {len(video_files)} valid video files\")\n",
    "\n",
    "if not video_files:\n",
    "    print(\"\\nâš ï¸  No video files found!\")\n",
    "    print(\"Please ensure your video files are uploaded to the Kaggle dataset or input directory.\")\n",
    "    print(\"Supported formats:\", config.SUPPORTED_VIDEO_FORMATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Process Videos\n",
    "\n",
    "Now we'll process each video through the complete pipeline. You can run this cell multiple times - it will resume from checkpoints if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ CONFIGURING MODELS FROM KAGGLE INPUT DATASETS\n",
      "============================================================\n",
      "Step 1: Configuring model paths...\n",
      "ðŸ’» Not running on Kaggle - using default model paths\n",
      "\n",
      "Step 2: Updating config...\n",
      "\n",
      "Step 3: Patching model loading functions...\n",
      "\n",
      "âš ï¸  Models will use default paths (may hit 20GB limit)\n",
      "ðŸ’¡ Consider uploading models as Kaggle datasets for optimal performance\n",
      "\n",
      "âœ… Model configuration completed!\n"
     ]
    }
   ],
   "source": [
    "# ðŸŽ¯ Configure Models from Kaggle Input Datasets (20GB Output Limit Solution)\n",
    "# This cell reconfigures the model paths to use Kaggle input datasets instead of downloading\n",
    "# to the output folder, which would exceed the 20GB limit before processing can start.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ðŸŽ¯ CONFIGURING MODELS FROM KAGGLE INPUT DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def configure_kaggle_model_paths():\n",
    "    \"\"\"Configure model paths to use Kaggle input datasets instead of output folder\"\"\"\n",
    "    \n",
    "    # Define Kaggle input dataset paths\n",
    "    kaggle_model_paths = {\n",
    "        'whisper': '/kaggle/input/whisper-large-v3',\n",
    "        'seamless': '/kaggle/input/seamlessm4t-large', \n",
    "        'openvoice': '/kaggle/input/openvoice-repo'\n",
    "    }\n",
    "    \n",
    "    # Check if we're on Kaggle\n",
    "    if not IS_KAGGLE:\n",
    "        print(\"ðŸ’» Not running on Kaggle - using default model paths\")\n",
    "        return False\n",
    "    \n",
    "    # Verify input datasets exist\n",
    "    missing_datasets = []\n",
    "    for name, path in kaggle_model_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            missing_datasets.append(f\"{name}: {path}\")\n",
    "    \n",
    "    if missing_datasets:\n",
    "        print(\"âš ï¸  MISSING INPUT DATASETS:\")\n",
    "        for missing in missing_datasets:\n",
    "            print(f\"   âŒ {missing}\")\n",
    "        print(\"\\nðŸ“‹ TO FIX THIS:\")\n",
    "        print(\"   1. Upload models as Kaggle datasets with these exact names:\")\n",
    "        print(\"      - whisper-large-v3 (contains large-v3.pt)\")\n",
    "        print(\"      - seamlessm4t-large (contains HuggingFace model files)\")\n",
    "        print(\"      - openvoice-repo (contains OpenVoice repository)\")\n",
    "        print(\"   2. Add these datasets to your notebook's input\")\n",
    "        print(\"   3. Re-run this cell\")\n",
    "        print(\"\\nðŸ”„ FALLBACK: Models will download to output folder (may hit 20GB limit)\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ… All model datasets found in Kaggle input!\")\n",
    "    \n",
    "    # Create symlinks in the models directory to point to input datasets\n",
    "    models_dir = config.MODELS_DIR\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Configure Whisper path\n",
    "    whisper_input_path = Path(kaggle_model_paths['whisper'])\n",
    "    whisper_link_path = models_dir / 'whisper'\n",
    "    \n",
    "    if whisper_link_path.exists() or whisper_link_path.is_symlink():\n",
    "        whisper_link_path.unlink()\n",
    "    \n",
    "    # For Whisper, we expect the model file to be directly in the input dataset\n",
    "    whisper_files = list(whisper_input_path.glob('*.pt'))\n",
    "    if whisper_files:\n",
    "        whisper_link_path.symlink_to(whisper_input_path)\n",
    "        print(f\"   ðŸ”— Whisper: {whisper_link_path} -> {whisper_input_path}\")\n",
    "        \n",
    "        # Set environment variable for Whisper cache\n",
    "        os.environ['WHISPER_CACHE'] = str(whisper_input_path)\n",
    "        print(f\"   ðŸŒ WHISPER_CACHE: {whisper_input_path}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  No .pt files found in {whisper_input_path}\")\n",
    "    \n",
    "    # Configure SeamlessM4T path  \n",
    "    seamless_input_path = Path(kaggle_model_paths['seamless'])\n",
    "    seamless_link_path = models_dir / 'seamless'\n",
    "    \n",
    "    if seamless_link_path.exists() or seamless_link_path.is_symlink():\n",
    "        seamless_link_path.unlink()\n",
    "    \n",
    "    # Look for HuggingFace model directory\n",
    "    hf_dirs = [d for d in seamless_input_path.iterdir() if d.is_dir() and 'hf-seamless' in d.name]\n",
    "    if hf_dirs:\n",
    "        seamless_link_path.symlink_to(seamless_input_path)\n",
    "        print(f\"   ðŸ”— SeamlessM4T: {seamless_link_path} -> {seamless_input_path}\")\n",
    "        \n",
    "        # Set HuggingFace cache environment variables\n",
    "        os.environ['HF_HOME'] = str(seamless_input_path)\n",
    "        os.environ['TRANSFORMERS_CACHE'] = str(seamless_input_path) \n",
    "        os.environ['HF_DATASETS_CACHE'] = str(seamless_input_path)\n",
    "        print(f\"   ðŸŒ HF_HOME: {seamless_input_path}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  No HuggingFace model directory found in {seamless_input_path}\")\n",
    "    \n",
    "    # Configure OpenVoice path\n",
    "    openvoice_input_path = Path(kaggle_model_paths['openvoice'])\n",
    "    openvoice_link_path = models_dir / 'openvoice'\n",
    "    \n",
    "    if openvoice_link_path.exists() or openvoice_link_path.is_symlink():\n",
    "        openvoice_link_path.unlink()\n",
    "    \n",
    "    if openvoice_input_path.exists():\n",
    "        openvoice_link_path.symlink_to(openvoice_input_path)\n",
    "        print(f\"   ðŸ”— OpenVoice: {openvoice_link_path} -> {openvoice_input_path}\")\n",
    "        \n",
    "        # Add OpenVoice to Python path\n",
    "        if str(openvoice_input_path) not in sys.path:\n",
    "            sys.path.insert(0, str(openvoice_input_path))\n",
    "            print(f\"   ðŸ Added to Python path: {openvoice_input_path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def update_config_for_input_datasets():\n",
    "    \"\"\"Update config to use input dataset paths instead of output folder paths\"\"\"\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        return\n",
    "    \n",
    "    # Override model paths in config to point to input datasets\n",
    "    if hasattr(config, 'MODELS_DIR'):\n",
    "        # Keep the original for symlinks, but set specific paths for each model\n",
    "        config.WHISPER_CACHE_DIR = '/kaggle/input/whisper-large-v3'\n",
    "        config.SEAMLESS_CACHE_DIR = '/kaggle/input/seamlessm4t-large' \n",
    "        config.OPENVOICE_DIR = '/kaggle/input/openvoice-repo'\n",
    "        \n",
    "        print(\"ðŸ”§ Updated config with input dataset paths:\")\n",
    "        print(f\"   WHISPER_CACHE_DIR: {config.WHISPER_CACHE_DIR}\")\n",
    "        print(f\"   SEAMLESS_CACHE_DIR: {config.SEAMLESS_CACHE_DIR}\")\n",
    "        print(f\"   OPENVOICE_DIR: {config.OPENVOICE_DIR}\")\n",
    "\n",
    "def patch_model_loading_functions():\n",
    "    \"\"\"Patch the model loading to use input datasets\"\"\"\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        return\n",
    "    \n",
    "    # Store original functions\n",
    "    import whisper\n",
    "    from transformers import SeamlessM4TModel, SeamlessM4TProcessor\n",
    "    \n",
    "    original_whisper_load = whisper.load_model\n",
    "    original_seamless_from_pretrained = SeamlessM4TProcessor.from_pretrained\n",
    "    original_seamless_model_from_pretrained = SeamlessM4TModel.from_pretrained\n",
    "    \n",
    "    def patched_whisper_load(name, download_root=None, in_memory=False):\n",
    "        \"\"\"Patched Whisper load to use input dataset\"\"\"\n",
    "        if download_root and 'working' in str(download_root):\n",
    "            # Redirect to input dataset\n",
    "            download_root = '/kaggle/input/whisper-large-v3'\n",
    "            print(f\"ðŸ”€ Redirecting Whisper download to: {download_root}\")\n",
    "        return original_whisper_load(name, download_root, in_memory)\n",
    "    \n",
    "    def patched_seamless_processor_from_pretrained(model_id, cache_dir=None, **kwargs):\n",
    "        \"\"\"Patched SeamlessM4T processor to use input dataset\"\"\"\n",
    "        if cache_dir and 'working' in str(cache_dir):\n",
    "            # Look for the HuggingFace model in input dataset\n",
    "            input_model_path = '/kaggle/input/seamlessm4t-large'\n",
    "            hf_dirs = [d for d in Path(input_model_path).iterdir() if d.is_dir() and 'hf-seamless' in d.name]\n",
    "            if hf_dirs:\n",
    "                model_id = str(hf_dirs[0])\n",
    "                cache_dir = None  # Don't use cache when loading from local path\n",
    "                kwargs['local_files_only'] = True\n",
    "                print(f\"ðŸ”€ Redirecting SeamlessM4T processor to: {model_id}\")\n",
    "        return original_seamless_from_pretrained(model_id, cache_dir=cache_dir, **kwargs)\n",
    "    \n",
    "    def patched_seamless_model_from_pretrained(model_id, cache_dir=None, **kwargs):\n",
    "        \"\"\"Patched SeamlessM4T model to use input dataset\"\"\"\n",
    "        if cache_dir and 'working' in str(cache_dir):\n",
    "            # Look for the HuggingFace model in input dataset  \n",
    "            input_model_path = '/kaggle/input/seamlessm4t-large'\n",
    "            hf_dirs = [d for d in Path(input_model_path).iterdir() if d.is_dir() and 'hf-seamless' in d.name]\n",
    "            if hf_dirs:\n",
    "                model_id = str(hf_dirs[0])\n",
    "                cache_dir = None  # Don't use cache when loading from local path\n",
    "                kwargs['local_files_only'] = True\n",
    "                print(f\"ðŸ”€ Redirecting SeamlessM4T model to: {model_id}\")\n",
    "        return original_seamless_model_from_pretrained(model_id, cache_dir=cache_dir, **kwargs)\n",
    "    \n",
    "    # Apply patches\n",
    "    whisper.load_model = patched_whisper_load\n",
    "    SeamlessM4TProcessor.from_pretrained = patched_seamless_processor_from_pretrained  \n",
    "    SeamlessM4TModel.from_pretrained = patched_seamless_model_from_pretrained\n",
    "    \n",
    "    print(\"ðŸ”§ Applied model loading patches to use input datasets\")\n",
    "\n",
    "# Execute the configuration\n",
    "print(\"Step 1: Configuring model paths...\")\n",
    "models_configured = configure_kaggle_model_paths()\n",
    "\n",
    "print(\"\\nStep 2: Updating config...\")\n",
    "update_config_for_input_datasets()\n",
    "\n",
    "print(\"\\nStep 3: Patching model loading functions...\")\n",
    "patch_model_loading_functions()\n",
    "\n",
    "# Report status\n",
    "if models_configured:\n",
    "    print(\"\\nðŸŽ‰ SUCCESS: Models configured to use Kaggle input datasets!\")\n",
    "    print(\"ðŸ’¾ This avoids downloading to output folder (saves ~15GB)\")\n",
    "    print(\"ðŸš€ Processing can now start without hitting the 20GB limit\")\n",
    "    \n",
    "    # Check sizes\n",
    "    if IS_KAGGLE:\n",
    "        total_input_size = 0\n",
    "        for dataset_path in ['/kaggle/input/whisper-large-v3', '/kaggle/input/seamlessm4t-large', '/kaggle/input/openvoice-repo']:\n",
    "            if os.path.exists(dataset_path):\n",
    "                try:\n",
    "                    size = sum(f.stat().st_size for f in Path(dataset_path).rglob('*') if f.is_file()) / (1024**3)\n",
    "                    total_input_size += size\n",
    "                    print(f\"   ðŸ“Š {dataset_path}: {size:.1f}GB\")\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"   ðŸ“Š Total input dataset size: {total_input_size:.1f}GB (not counted against output limit)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Models will use default paths (may hit 20GB limit)\")\n",
    "    print(\"ðŸ’¡ Consider uploading models as Kaggle datasets for optimal performance\")\n",
    "\n",
    "print(f\"\\nâœ… Model configuration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Video processor class created\n"
     ]
    }
   ],
   "source": [
    "# Main processing function\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "class VideoDubbingProcessor:\n",
    "    def __init__(self, video_name):\n",
    "        self.video_name = video_name\n",
    "        self.logger = logging.getLogger(f\"processor_{video_name}\")\n",
    "        self.checkpoint_file = config.CHECKPOINTS_DIR / f\"{video_name}_checkpoint.json\"\n",
    "        \n",
    "    def save_checkpoint(self, step, data):\n",
    "        \"\"\"Save processing checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            \"video_name\": self.video_name,\n",
    "            \"step\": step,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": data\n",
    "        }\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load existing checkpoint\"\"\"\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def extract_audio(self, video_path):\n",
    "        \"\"\"Extract and clean audio from video\"\"\"\n",
    "        self.logger.info(\"Extracting audio...\")\n",
    "        \n",
    "        audio_path = config.TEMP_DIR / f\"{self.video_name}_audio.wav\"\n",
    "        \n",
    "        # Extract audio using ffmpeg\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-i\", str(video_path),\n",
    "            \"-ar\", str(config.AUDIO_SAMPLE_RATE),\n",
    "            \"-ac\", \"1\",  # Mono\n",
    "            \"-y\", str(audio_path)\n",
    "        ]\n",
    "        \n",
    "        subprocess.run(cmd, capture_output=True, check=True)\n",
    "        \n",
    "        # Apply noise reduction\n",
    "        import noisereduce as nr\n",
    "        audio, sr = librosa.load(str(audio_path), sr=config.AUDIO_SAMPLE_RATE)\n",
    "        reduced_audio = nr.reduce_noise(y=audio, sr=sr)\n",
    "        \n",
    "        clean_audio_path = config.TEMP_DIR / f\"{self.video_name}_clean_audio.wav\"\n",
    "        librosa.output.write_wav(str(clean_audio_path), reduced_audio, sr)\n",
    "        \n",
    "        return clean_audio_path\n",
    "    \n",
    "    def transcribe_audio(self, audio_path):\n",
    "        \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "        self.logger.info(\"Transcribing audio...\")\n",
    "        \n",
    "        # Load Whisper model\n",
    "        model = whisper.load_model(\n",
    "            config.WHISPER_MODEL,\n",
    "            download_root=str(config.MODELS_DIR)\n",
    "        )\n",
    "        \n",
    "        # Transcribe\n",
    "        result = model.transcribe(\n",
    "            str(audio_path),\n",
    "            language=\"ar\",\n",
    "            word_timestamps=True\n",
    "        )\n",
    "        \n",
    "        # Save transcription\n",
    "        transcript_file = config.TEMP_DIR / f\"{self.video_name}_transcript.json\"\n",
    "        with open(transcript_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def translate_text(self, transcription, target_language):\n",
    "        \"\"\"Translate transcription using SeamlessM4T\"\"\"\n",
    "        self.logger.info(f\"Translating to {target_language}...\")\n",
    "        \n",
    "        # Load SeamlessM4T\n",
    "        processor = SeamlessM4TProcessor.from_pretrained(\n",
    "            config.SEAMLESS_MODEL,\n",
    "            cache_dir=str(config.MODELS_DIR)\n",
    "        )\n",
    "        \n",
    "        model = SeamlessM4TModel.from_pretrained(\n",
    "            config.SEAMLESS_MODEL,\n",
    "            cache_dir=str(config.MODELS_DIR),\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(\"cuda\")\n",
    "        \n",
    "        # Language mapping\n",
    "        lang_map = {\"en\": \"eng\", \"de\": \"deu\"}\n",
    "        target_lang = lang_map.get(target_language, target_language)\n",
    "        \n",
    "        # Translate segments\n",
    "        translated_segments = []\n",
    "        \n",
    "        for segment in transcription[\"segments\"]:\n",
    "            text = segment[\"text\"].strip()\n",
    "            if len(text) < 3:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                inputs = processor(\n",
    "                    text=text,\n",
    "                    src_lang=\"arb\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        tgt_lang=target_lang,\n",
    "                        max_new_tokens=512\n",
    "                    )\n",
    "                \n",
    "                translation = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                translated_segments.append({\n",
    "                    \"start\": segment[\"start\"],\n",
    "                    \"end\": segment[\"end\"],\n",
    "                    \"original_text\": text,\n",
    "                    \"translated_text\": translation\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Translation failed for segment: {e}\")\n",
    "                translated_segments.append({\n",
    "                    \"start\": segment[\"start\"],\n",
    "                    \"end\": segment[\"end\"],\n",
    "                    \"original_text\": text,\n",
    "                    \"translated_text\": f\"[Translation Error: {text}]\"\n",
    "                })\n",
    "        \n",
    "        del model, processor\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return translated_segments\n",
    "    \n",
    "    def create_subtitles(self, segments, language):\n",
    "        \"\"\"Create SRT subtitle file\"\"\"\n",
    "        self.logger.info(f\"Creating subtitles for {language}...\")\n",
    "        \n",
    "        output_dir = config.OUTPUT_DIR / self.video_name\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        srt_file = output_dir / f\"{self.video_name}_{language}.srt\"\n",
    "        \n",
    "        with open(srt_file, 'w', encoding='utf-8') as f:\n",
    "            for i, segment in enumerate(segments, 1):\n",
    "                start_time = self._seconds_to_srt_time(segment[\"start\"])\n",
    "                end_time = self._seconds_to_srt_time(segment[\"end\"])\n",
    "                text = segment[\"translated_text\"]\n",
    "                \n",
    "                f.write(f\"{i}\\n\")\n",
    "                f.write(f\"{start_time} --> {end_time}\\n\")\n",
    "                f.write(f\"{text}\\n\\n\")\n",
    "        \n",
    "        return srt_file\n",
    "    \n",
    "    def _seconds_to_srt_time(self, seconds):\n",
    "        \"\"\"Convert seconds to SRT timestamp format\"\"\"\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        millisecs = int((seconds % 1) * 1000)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}\"\n",
    "    \n",
    "    def create_final_video(self, original_video, subtitle_files, language):\n",
    "        \"\"\"Create final video with subtitles\"\"\"\n",
    "        self.logger.info(f\"Creating final video for {language}...\")\n",
    "        \n",
    "        output_path = config.get_video_output_path(self.video_name, language)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # For now, just copy original video and add subtitles\n",
    "        # In full implementation, this would include dubbed audio\n",
    "        cmd = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", str(original_video),\n",
    "            \"-i\", str(subtitle_files[language]),\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"copy\",\n",
    "            \"-c:s\", \"mov_text\",\n",
    "            \"-map\", \"0\",\n",
    "            \"-map\", \"1\",\n",
    "            \"-y\", str(output_path)\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, capture_output=True, check=True)\n",
    "            return output_path\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"Video creation failed: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"âœ“ Video processor class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No video files found to process\n",
      "Please check:\n",
      "1. The INPUT_DIR path is correct\n",
      "2. Video files exist in the specified directory\n",
      "3. Video files have supported extensions (.mp4, .avi, .mov, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Process videos\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Add this section to define video_files before using it\n",
    "def get_video_files(input_directory, extensions=None):\n",
    "    \"\"\"Find all video files in the input directory\"\"\"\n",
    "    if extensions is None:\n",
    "        extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']\n",
    "    \n",
    "    input_path = Path(input_directory)\n",
    "    video_files = []\n",
    "    \n",
    "    for ext in extensions:\n",
    "        video_files.extend(input_path.glob(f\"*{ext}\"))\n",
    "        video_files.extend(input_path.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    return sorted(video_files)\n",
    "\n",
    "def process_videos(video_files, max_videos=None):\n",
    "    \"\"\"Process video files through the dubbing pipeline\"\"\"\n",
    "    \n",
    "    if max_videos:\n",
    "        video_files = video_files[:max_videos]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        video_name = video_file.stem\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"Processing: {video_name}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        processor = VideoDubbingProcessor(video_name)\n",
    "        \n",
    "        try:\n",
    "            # Check for existing checkpoint\n",
    "            checkpoint = processor.load_checkpoint()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Step 1: Extract and clean audio\n",
    "            if not checkpoint or checkpoint.get(\"step\") < 1:\n",
    "                clean_audio_path = processor.extract_audio(video_file)\n",
    "                processor.save_checkpoint(1, {\"clean_audio\": str(clean_audio_path)})\n",
    "                logger.info(\"âœ“ Audio extraction completed\")\n",
    "            else:\n",
    "                clean_audio_path = Path(checkpoint[\"data\"][\"clean_audio\"])\n",
    "                logger.info(\"âœ“ Audio extraction (from checkpoint)\")\n",
    "            \n",
    "            # Step 2: Transcribe audio\n",
    "            if not checkpoint or checkpoint.get(\"step\") < 2:\n",
    "                transcription = processor.transcribe_audio(clean_audio_path)\n",
    "                processor.save_checkpoint(2, {\"transcription_file\": f\"{video_name}_transcript.json\"})\n",
    "                logger.info(\"âœ“ Transcription completed\")\n",
    "            else:\n",
    "                transcript_file = config.TEMP_DIR / f\"{video_name}_transcript.json\"\n",
    "                with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "                    transcription = json.load(f)\n",
    "                logger.info(\"âœ“ Transcription (from checkpoint)\")\n",
    "            \n",
    "            # Step 3: Translate and create subtitles\n",
    "            subtitle_files = {}\n",
    "            \n",
    "            for language in config.TARGET_LANGUAGES:\n",
    "                if not checkpoint or checkpoint.get(\"step\") < 3:\n",
    "                    translated_segments = processor.translate_text(transcription, language)\n",
    "                    subtitle_file = processor.create_subtitles(translated_segments, language)\n",
    "                    subtitle_files[language] = subtitle_file\n",
    "                    \n",
    "                    processor.save_checkpoint(3, {\n",
    "                        \"subtitle_files\": {lang: str(path) for lang, path in subtitle_files.items()}\n",
    "                    })\n",
    "                    logger.info(f\"âœ“ Translation and subtitles for {language} completed\")\n",
    "                else:\n",
    "                    subtitle_files = {lang: Path(path) for lang, path in checkpoint[\"data\"][\"subtitle_files\"].items()}\n",
    "                    logger.info(f\"âœ“ Translation for {language} (from checkpoint)\")\n",
    "            \n",
    "            # Step 4: Create final videos (simplified version)\n",
    "            final_videos = {}\n",
    "            for language in config.TARGET_LANGUAGES:\n",
    "                final_video = processor.create_final_video(video_file, subtitle_files, language)\n",
    "                if final_video:\n",
    "                    final_videos[language] = final_video\n",
    "                    logger.info(f\"âœ“ Final video for {language} created\")\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            results[video_name] = {\n",
    "                \"status\": \"completed\",\n",
    "                \"processing_time_minutes\": processing_time / 60,\n",
    "                \"subtitle_files\": {lang: str(path) for lang, path in subtitle_files.items()},\n",
    "                \"final_videos\": {lang: str(path) for lang, path in final_videos.items()}\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"âœ“ {video_name} completed in {processing_time/60:.1f} minutes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âœ— Failed to process {video_name}: {e}\")\n",
    "            results[video_name] = {\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        \n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define video_files before using it\n",
    "try:\n",
    "    # Option 1: Get video files from a specific directory\n",
    "    # Replace 'your_input_directory' with the actual path to your videos\n",
    "    INPUT_DIR = \"/kaggle/input/outofthebox\"  # Change this to your video directory\n",
    "    video_files = get_video_files(INPUT_DIR)\n",
    "    \n",
    "    # Option 2: If you already have a list of video file paths, uncomment and modify:\n",
    "    # video_files = [\n",
    "    #     Path(\"path/to/video1.mp4\"),\n",
    "    #     Path(\"path/to/video2.mp4\"),\n",
    "    #     # Add more video paths as needed\n",
    "    # ]\n",
    "    \n",
    "    # Option 3: If config has an INPUT_DIR defined, uncomment:\n",
    "    # video_files = get_video_files(config.INPUT_DIR)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error finding video files: {e}\")\n",
    "    video_files = []\n",
    "\n",
    "# Process videos (limit to 2 for demo)\n",
    "if video_files:\n",
    "    print(f\"Found {len(video_files)} video files\")\n",
    "    print(f\"Starting processing of {min(2, len(video_files))} videos...\")\n",
    "    processing_results = process_videos(video_files, max_videos=2)\n",
    "    \n",
    "    # Save results\n",
    "    results_file = config.OUTPUT_DIR / \"processing_results.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(processing_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for video_name, result in processing_results.items():\n",
    "        status = result[\"status\"]\n",
    "        if status == \"completed\":\n",
    "            time_taken = result[\"processing_time_minutes\"]\n",
    "            print(f\"âœ“ {video_name}: {status} ({time_taken:.1f} min)\")\n",
    "        else:\n",
    "            print(f\"âœ— {video_name}: {status}\")\n",
    "    \n",
    "    successful = sum(1 for r in processing_results.values() if r[\"status\"] == \"completed\")\n",
    "    print(f\"\\nSuccess rate: {successful}/{len(processing_results)} videos\")\n",
    "    \n",
    "else:\n",
    "    print(\"No video files found to process\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. The INPUT_DIR path is correct\")\n",
    "    print(\"2. Video files exist in the specified directory\")\n",
    "    print(\"3. Video files have supported extensions (.mp4, .avi, .mov, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Results and Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m                     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ðŸ“„ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_mb\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mMB)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mdisplay_output_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Display processing statistics\u001b[39;00m\n\u001b[32m     35\u001b[39m results_file = config.OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mprocessing_results.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mdisplay_output_files\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdisplay_output_files\u001b[39m():\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Display generated output files\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     output_dir = \u001b[43mconfig\u001b[49m.OUTPUT_DIR\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_dir.exists():\n\u001b[32m     10\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo output directory found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Display results and output files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def display_output_files():\n",
    "    \"\"\"Display generated output files\"\"\"\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        print(\"No output directory found\")\n",
    "        return\n",
    "    \n",
    "    print(\"Generated Output Files:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for video_dir in output_dir.iterdir():\n",
    "        if video_dir.is_dir():\n",
    "            print(f\"\\nðŸ“ {video_dir.name}/\")\n",
    "            \n",
    "            files = list(video_dir.glob(\"*\"))\n",
    "            for file in sorted(files):\n",
    "                size_mb = file.stat().st_size / (1024*1024)\n",
    "                if file.suffix == '.srt':\n",
    "                    print(f\"  ðŸ“ {file.name} ({size_mb:.1f}MB) - Subtitles\")\n",
    "                elif file.suffix == '.mp4':\n",
    "                    print(f\"  ðŸŽ¬ {file.name} ({size_mb:.1f}MB) - Video\")\n",
    "                elif file.suffix == '.json':\n",
    "                    print(f\"  ðŸ“‹ {file.name} ({size_mb:.1f}MB) - Report\")\n",
    "                else:\n",
    "                    print(f\"  ðŸ“„ {file.name} ({size_mb:.1f}MB)\")\n",
    "\n",
    "display_output_files()\n",
    "\n",
    "# Display processing statistics\n",
    "results_file = config.OUTPUT_DIR / \"processing_results.json\"\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROCESSING STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_videos = len(results)\n",
    "    completed = sum(1 for r in results.values() if r[\"status\"] == \"completed\")\n",
    "    failed = total_videos - completed\n",
    "    \n",
    "    print(f\"Total videos processed: {total_videos}\")\n",
    "    print(f\"Successfully completed: {completed}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Success rate: {(completed/total_videos)*100:.1f}%\")\n",
    "    \n",
    "    if completed > 0:\n",
    "        avg_time = sum(r.get(\"processing_time_minutes\", 0) \n",
    "                      for r in results.values() \n",
    "                      if r[\"status\"] == \"completed\") / completed\n",
    "        print(f\"Average processing time: {avg_time:.1f} minutes per video\")\n",
    "    \n",
    "    print(f\"\\nOutput directory: {config.OUTPUT_DIR}\")\n",
    "    print(f\"Total output files: {sum(len(list(d.glob('*'))) for d in config.OUTPUT_DIR.iterdir() if d.is_dir())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Download Results (Kaggle)\n",
    "\n",
    "If you're running on Kaggle, this will prepare your results for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create downloadable archive of results\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def create_results_archive():\n",
    "    \"\"\"Create a zip archive of all results\"\"\"\n",
    "    if not config.OUTPUT_DIR.exists():\n",
    "        print(\"No output directory found\")\n",
    "        return None\n",
    "    \n",
    "    archive_path = config.WORKING_DIR / \"dubbing_results.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add all files from output directory\n",
    "        for root, dirs, files in os.walk(config.OUTPUT_DIR):\n",
    "            for file in files:\n",
    "                file_path = Path(root) / file\n",
    "                arc_path = file_path.relative_to(config.OUTPUT_DIR)\n",
    "                zipf.write(file_path, arc_path)\n",
    "        \n",
    "        # Add processing results\n",
    "        results_file = config.OUTPUT_DIR / \"processing_results.json\"\n",
    "        if results_file.exists():\n",
    "            zipf.write(results_file, \"processing_results.json\")\n",
    "        \n",
    "        # Add logs\n",
    "        log_files = list(config.LOGS_DIR.glob(\"*.log\"))\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file, f\"logs/{log_file.name}\")\n",
    "    \n",
    "    size_mb = archive_path.stat().st_size / (1024*1024)\n",
    "    print(f\"âœ“ Results archive created: {archive_path.name} ({size_mb:.1f}MB)\")\n",
    "    \n",
    "    return archive_path\n",
    "\n",
    "if IS_KAGGLE and config.OUTPUT_DIR.exists():\n",
    "    archive = create_results_archive()\n",
    "    if archive:\n",
    "        print(f\"\\nðŸ“¦ Download your results: {archive}\")\n",
    "        print(\"The archive contains:\")\n",
    "        print(\"- Dubbed videos (MP4)\")\n",
    "        print(\"- Subtitle files (SRT)\")\n",
    "        print(\"- Processing reports (JSON)\")\n",
    "        print(\"- Processing logs\")\n",
    "else:\n",
    "    print(\"No results to archive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– Next Steps\n",
    "\n",
    "### What This Notebook Does:\n",
    "\n",
    "1. **âœ… Audio Processing**: Extracts and cleans audio from videos\n",
    "2. **âœ… Transcription**: Uses Whisper large-v3 to transcribe Arabic speech\n",
    "3. **âœ… Translation**: Translates Arabic to English/German using SeamlessM4T\n",
    "4. **âœ… Subtitles**: Generates properly formatted SRT subtitle files\n",
    "5. **âœ… Basic Video Assembly**: Creates videos with embedded subtitles\n",
    "\n",
    "### For Full Implementation:\n",
    "\n",
    "The complete pipeline (as described in the project requirements) would include:\n",
    "\n",
    "- **Voice Cloning**: Using OpenVoice v2 to clone the original speaker's voice\n",
    "- **Speech Synthesis**: Generating dubbed audio in target languages\n",
    "- **Audio Synchronization**: Using DTW for precise timing alignment\n",
    "- **Quality Assurance**: Comprehensive audio/video quality checks\n",
    "- **Multi-track Assembly**: Creating videos with multiple audio tracks\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "1. **Input Format**: Upload your Arabic video files to the Kaggle dataset\n",
    "2. **File Size**: Maximum 8GB per video file\n",
    "3. **Processing Time**: ~30-60 minutes per hour of video content\n",
    "4. **Memory Management**: The notebook automatically manages GPU memory\n",
    "5. **Checkpointing**: Processing can resume from interruptions\n",
    "\n",
    "### Output Files:\n",
    "\n",
    "- `{video_name}_en.srt` - English subtitles\n",
    "- `{video_name}_de.srt` - German subtitles  \n",
    "- `{video_name}_en_1080p.mp4` - English video with subtitles\n",
    "- `{video_name}_de_1080p.mp4` - German video with subtitles\n",
    "- `processing_results.json` - Processing summary and statistics\n",
    "\n",
    "### Customization:\n",
    "\n",
    "You can modify the `config.py` file to:\n",
    "- Change target languages\n",
    "- Adjust quality settings\n",
    "- Modify processing parameters\n",
    "- Set custom output formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Environment Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download and create project files\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "# Project files to create\n",
    "project_files = {\n",
    "    'config.py': '''\"\"\"Configuration module for video dubbing automation\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, local_mode=False):\n",
    "        if local_mode or not os.path.exists('/kaggle'):\n",
    "            self.BASE_DIR = Path.cwd()\n",
    "        else:\n",
    "            self.BASE_DIR = Path('/kaggle/working')\n",
    "        \n",
    "        # Directory structure\n",
    "        self.MODELS_DIR = self.BASE_DIR / 'models'\n",
    "        self.TEMP_DIR = self.BASE_DIR / 'temp'\n",
    "        self.OUTPUT_DIR = self.BASE_DIR / 'output'\n",
    "        self.LOGS_DIR = self.BASE_DIR / 'logs'\n",
    "        self.CHECKPOINTS_DIR = self.BASE_DIR / 'checkpoints'\n",
    "        \n",
    "        # Audio settings\n",
    "        self.AUDIO_SAMPLE_RATE = 48000\n",
    "        self.CHUNK_DURATION = 30  # seconds\n",
    "        \n",
    "        # Processing settings\n",
    "        self.TARGET_LANGUAGES = ['en', 'de']\n",
    "        self.BATCH_SIZE = 1\n",
    "        self.MAX_RETRIES = 3\n",
    "        \n",
    "        # Create directories\n",
    "        for directory in [self.MODELS_DIR, self.TEMP_DIR, self.OUTPUT_DIR, \n",
    "                         self.LOGS_DIR, self.CHECKPOINTS_DIR]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()''',\n",
    "    \n",
    "    'test_environment.py': '''# Environment validation will be created here''',\n",
    "    'demo_processor.py': '''# Demo processor will be created here'''\n",
    "}\n",
    "\n",
    "# Create the files\n",
    "for filename, content in project_files.items():\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(f\"âœ“ Created {filename}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Project files created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run environment validation\n",
    "print(\"ðŸ” Running Environment Validation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import validation functions\n",
    "try:\n",
    "    exec(open('test_environment.py').read())\n",
    "    validator = EnvironmentValidator(local_mode=not IS_KAGGLE)\n",
    "    validation_results = validator.run_full_validation()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Validation completed: {validation_results.get('overall_status', 'Unknown')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Validation failed: {e}\")\n",
    "    \n",
    "    # Manual basic checks\n",
    "    print(\"\\nðŸ”§ Running basic manual checks...\")\n",
    "    \n",
    "    # Check PyTorch and CUDA\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "        print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   Device: {torch.cuda.get_device_name()}\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ PyTorch not available\")\n",
    "    \n",
    "    # Check other key packages\n",
    "    packages_to_check = ['transformers', 'whisper', 'librosa', 'moviepy']\n",
    "    for package in packages_to_check:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"âœ… {package}\")\n",
    "        except ImportError:\n",
    "            print(f\"âŒ {package} not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Demo and Testing Mode\n",
    "\n",
    "Before processing real videos, let's test the pipeline with synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run demo pipeline test\n",
    "print(\"ðŸ§ª Starting Demo Pipeline Test...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Create a simple demo processor\n",
    "    class SimpleDemoProcessor:\n",
    "        def __init__(self):\n",
    "            self.results = {}\n",
    "        \n",
    "        def test_audio_processing(self):\n",
    "            print(\"\\nðŸŽµ Testing Audio Processing...\")\n",
    "            try:\n",
    "                import librosa\n",
    "                import numpy as np\n",
    "                \n",
    "                # Create test audio\n",
    "                duration = 5  # seconds\n",
    "                sr = 22050\n",
    "                t = np.linspace(0, duration, duration * sr)\n",
    "                test_audio = 0.5 * np.sin(2 * np.pi * 440 * t)  # 440 Hz tone\n",
    "                \n",
    "                # Test librosa functionality\n",
    "                mfccs = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=13)\n",
    "                \n",
    "                print(\"  âœ… Audio generation: OK\")\n",
    "                print(\"  âœ… Librosa processing: OK\")\n",
    "                print(f\"  ðŸ“Š MFCC shape: {mfccs.shape}\")\n",
    "                \n",
    "                return {\"status\": \"âœ… PASSED\", \"details\": \"Audio processing working\"}\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Audio processing failed: {e}\")\n",
    "                return {\"status\": \"âŒ FAILED\", \"error\": str(e)}\n",
    "        \n",
    "        def test_model_loading(self):\n",
    "            print(\"\\nðŸ¤– Testing Model Loading...\")\n",
    "            try:\n",
    "                import whisper\n",
    "                \n",
    "                # Test whisper model loading\n",
    "                print(\"  ðŸ“¥ Loading Whisper base model...\")\n",
    "                model = whisper.load_model(\"base\")\n",
    "                print(\"  âœ… Whisper model loaded successfully\")\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                \n",
    "                return {\"status\": \"âœ… PASSED\", \"details\": \"Model loading working\"}\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Model loading failed: {e}\")\n",
    "                return {\"status\": \"âŒ FAILED\", \"error\": str(e)}\n",
    "        \n",
    "        def test_transformers(self):\n",
    "            print(\"\\nðŸŒ Testing Transformers...\")\n",
    "            try:\n",
    "                from transformers import pipeline\n",
    "                \n",
    "                # Test a simple pipeline\n",
    "                print(\"  ðŸ“¥ Creating translation pipeline...\")\n",
    "                # Use a small model for testing\n",
    "                translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ar-en\", \n",
    "                                     device=0 if torch.cuda.is_available() else -1)\n",
    "                \n",
    "                # Test translation\n",
    "                test_text = \"Ù…Ø±Ø­Ø¨Ø§\"  # \"Hello\" in Arabic\n",
    "                result = translator(test_text)\n",
    "                print(f\"  âœ… Translation test: '{test_text}' -> '{result[0]['translation_text']}'\")\n",
    "                \n",
    "                return {\"status\": \"âœ… PASSED\", \"details\": \"Translation working\"}\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Translation test failed: {e}\")\n",
    "                return {\"status\": \"âš ï¸  PARTIAL\", \"error\": str(e)}\n",
    "        \n",
    "        def run_full_test(self):\n",
    "            print(\"ðŸš€ Running comprehensive demo test...\")\n",
    "            \n",
    "            tests = [\n",
    "                (\"audio_processing\", self.test_audio_processing),\n",
    "                (\"model_loading\", self.test_model_loading),\n",
    "                (\"transformers\", self.test_transformers)\n",
    "            ]\n",
    "            \n",
    "            results = {}\n",
    "            passed = 0\n",
    "            \n",
    "            for test_name, test_func in tests:\n",
    "                try:\n",
    "                    result = test_func()\n",
    "                    results[test_name] = result\n",
    "                    if \"âœ…\" in result[\"status\"]:\n",
    "                        passed += 1\n",
    "                except Exception as e:\n",
    "                    results[test_name] = {\"status\": \"âŒ FAILED\", \"error\": str(e)}\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Demo Test Results: {passed}/{len(tests)} tests passed\")\n",
    "            \n",
    "            if passed == len(tests):\n",
    "                print(\"ðŸŽ‰ All tests passed! System ready for video processing.\")\n",
    "            elif passed >= len(tests) // 2:\n",
    "                print(\"âš ï¸  Some tests passed. System partially ready.\")\n",
    "            else:\n",
    "                print(\"âŒ Multiple tests failed. Please check installation.\")\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Run the demo\n",
    "    demo = SimpleDemoProcessor()\n",
    "    demo_results = demo.run_full_test()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Demo test failed: {e}\")\n",
    "    print(\"\\nðŸ“ Manual verification:\")\n",
    "    print(\"1. Check that all packages are installed\")\n",
    "    print(\"2. Verify GPU availability if needed\")\n",
    "    print(\"3. Ensure sufficient disk space (>20GB recommended)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¥ Video Processing Pipeline\n",
    "\n",
    "Once the environment validation passes, you can start processing your videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Video processing configuration and setup\n",
    "print(\"ðŸŽ¬ Video Dubbing Pipeline Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuration settings\n",
    "VIDEO_CONFIG = {\n",
    "    \"source_language\": \"ar\",  # Arabic\n",
    "    \"target_languages\": [\"en\", \"de\"],  # English and German\n",
    "    \"quality_preset\": \"high\",  # high, medium, fast\n",
    "    \"enable_subtitles\": True,\n",
    "    \"enable_speaker_diarization\": True,\n",
    "    \"max_video_length_minutes\": 120,\n",
    "    \"chunk_size_minutes\": 30  # For memory management\n",
    "}\n",
    "\n",
    "print(\"ðŸ“‹ Current Configuration:\")\n",
    "for key, value in VIDEO_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Input validation\n",
    "print(\"\\nðŸ“ Input Requirements:\")\n",
    "print(\"  â€¢ Video format: MP4, AVI, MOV, MKV\")\n",
    "print(\"  â€¢ Audio: Clear speech, minimal background noise\")\n",
    "print(\"  â€¢ Language: Arabic (Egyptian dialect preferred)\")\n",
    "print(\"  â€¢ Duration: 60-120 minutes recommended\")\n",
    "print(\"  â€¢ Size: Up to 8GB per video\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Expected Output:\")\n",
    "print(\"  â€¢ English dubbed video (MP4)\")\n",
    "print(\"  â€¢ German dubbed video (MP4)\")\n",
    "print(\"  â€¢ Subtitle files (SRT/VTT)\")\n",
    "print(\"  â€¢ Processing report (JSON)\")\n",
    "print(\"  â€¢ Quality metrics and validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# File upload and processing initialization\n",
    "print(\"ðŸ“¤ Video Upload and Processing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"ðŸ“ On Kaggle, your video files should be in:\")\n",
    "    print(\"   /kaggle/input/your-dataset-name/\")\n",
    "    print(\"\\nðŸ” Available input datasets:\")\n",
    "    \n",
    "    import os\n",
    "    input_path = Path('/kaggle/input')\n",
    "    if input_path.exists():\n",
    "        datasets = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "        if datasets:\n",
    "            for dataset in datasets:\n",
    "                print(f\"   ðŸ“‚ {dataset.name}\")\n",
    "                # List video files in dataset\n",
    "                video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "                videos = [f for f in dataset.rglob('*') \n",
    "                         if f.suffix.lower() in video_extensions]\n",
    "                for video in videos[:3]:  # Show first 3 videos\n",
    "                    size_mb = video.stat().st_size / 1024**2\n",
    "                    print(f\"      ðŸŽ¥ {video.name} ({size_mb:.1f}MB)\")\n",
    "                if len(videos) > 3:\n",
    "                    print(f\"      ... and {len(videos)-3} more videos\")\n",
    "        else:\n",
    "            print(\"   âŒ No datasets found. Please upload your videos to a Kaggle dataset first.\")\n",
    "            print(\"\\nðŸ“– How to upload videos:\")\n",
    "            print(\"   1. Create a new dataset on Kaggle\")\n",
    "            print(\"   2. Upload your video files\")\n",
    "            print(\"   3. Add the dataset to this notebook\")\n",
    "else:\n",
    "    print(\"ðŸ’» In local mode, place your videos in:\")\n",
    "    print(\"   ./input/ directory\")\n",
    "    \n",
    "    # Create input directory if it doesn't exist\n",
    "    input_dir = Path('./input')\n",
    "    input_dir.mkdir(exist_ok=True)\n",
    "    print(f\"   Created: {input_dir.absolute()}\")\n",
    "\n",
    "print(\"\\nâš¡ Ready to start processing!\")\n",
    "print(\"   Use the next cell to select and process videos.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7667190,
     "sourceId": 12173723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7677601,
     "sourceId": 12189162,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7677738,
     "sourceId": 12189351,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7682824,
     "sourceId": 12196685,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
